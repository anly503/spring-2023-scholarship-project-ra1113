[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Data Job Market in the US",
    "section": "",
    "text": "Inspired to learn more about the U.S. banking system following Silicon Valley Bank’s failure in March 2023, we used 30 years of FDIC data to visualize the components of the U.S. banking system we found most interesting. We visualized U.S. bank assets’ locations using historical financial data obtained through the FDIC API, finding some surprising results. Next, we dive deeper into why bank geographical locations resulted in the configuration. From there, we sought further to explore the changes within the concentration of assets and demonstrate how big banks have grown to hold a far more significant proportion of assets than they did 30 years ago. Finally, we used the equity capital ratio to understand the differing needs of various banks and how they can improve their financial position to respond to potential problems.\n\n\n\nOn Friday, March 10th, 2023, U.S. regulators seized Silicon Valley Bank (SVB) in the largest U.S. bank failure since the 2008 financial crisis [@vanian_heres_2023]. The 40-year-long bank operation suddenly stopped after depositors panicked when learning that the bank was short on capital. The public quickly turned its attention to the Federal Deposit Insurance Corporation (FDIC), which provides a standard insurance amount of $250,000 per depositor, per insured bank. The insurance fund has accomplished the goal of no depositor losing a penny of insured funds due to a failure since 1934 [@federal_deposit_insurance_corporation_what_2020]. The case of the FDIC showcased just one of the fascinating aspects of the U.S. banking system that was largely unknown to the general public. It also inspired us to carry out this project.\nWe aim to inform our audience of several distinguishing facts about the U.S. banking system and to do so in a visually captivating manner. First, we seek to provide insight into the complexity of where money is in the U.S. and what it means to have a location in this context. Then, we delve into the distribution of assets in the banking system and explore their concentration. Next, we switch our discussion to deposits and visualize their behavior over time. Lastly, we navigate bank health metrics and analyze the state of the U.S. banking sector.\nFirst, we explore the average assets, equity, or liabilities segmented of each as of the end of the fourth quarter of 2022 Figure 1:\n\n\n\n\n\n\n\n        \n        \nFigure 1: shows an interactive table with a dropdown allowing the selection of assets, equity, or liabilities. Given the static nature of this website, the table is sorted only by Equity.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing this interactive data table, we can obtain a broad view of the US banking system size by several metrics, setting the stage for the following sections."
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import os\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\nimport re\nimport numpy as np"
  },
  {
    "objectID": "data-cleaning.html#import-and-merge-json-files",
    "href": "data-cleaning.html#import-and-merge-json-files",
    "title": "Data Cleaning",
    "section": "1 Import and Merge JSON files",
    "text": "1 Import and Merge JSON files\nThe first step is to import all the JSON files from the 2 folders relating to DC jobs and all other jobs. We will have to import all the JSON files in each folder, merge them together, and merge the output of both folders, before converting it to a pandas DataFrame\n\ndef merge_jobs_results(directory): # merge all jsons from a folder\n    # initialize an empty list to hold the dfs\n    dfs = []\n\n    # iterate over all files in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            # read in the json data from the file\n            filepath = os.path.join(directory, filename)\n            with open(filepath, 'r') as f:\n                json_data = json.load(f)\n\n            # extract 'jobs_results' and convert to a df\n            jobs_data = json_data.get('jobs_results', [])\n            df = pd.json_normalize(jobs_data)\n            df['key'] = filename[:-7] # add column for search key, taken from file name\n\n            # add the df to the list\n            dfs.append(df)\n\n    # concatenate into a single df\n    merged_df = pd.concat(dfs, ignore_index=True)\n\n    return merged_df    \n\n\nmerged = merge_jobs_results('../../data/2023-04-14-job-search/2023-04-14-job-search-location-DC/') # DC jobs\n\n\nmerged = merged.drop('detected_extensions.commute_time', axis=1).copy() # drop last column so that both folders' files' dimensions match\n\nmerged.head(3)\n\n\n\n\n\n  \n    \n      \n      title\n      company_name\n      location\n      via\n      description\n      job_highlights\n      related_links\n      extensions\n      job_id\n      detected_extensions.schedule_type\n      detected_extensions.work_from_home\n      detected_extensions.posted_at\n      detected_extensions.salary\n      key\n    \n  \n  \n    \n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      via Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      [{'title': 'Qualifications', 'items': ['2-3 ye...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [Work from home, Full-time, No degree mentioned]\n      eyJqb2JfdGl0bGUiOiJFdGhlcmV1bSBCbG9ja2NoYWluIE...\n      Full-time\n      True\n      NaN\n      NaN\n      block-chain\n    \n    \n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      via Greenhouse\n      We are seeking a highly motivated and skilled ...\n      [{'title': 'Qualifications', 'items': ['Bachel...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [Full-time]\n      eyJqb2JfdGl0bGUiOiJCbG9ja2NoYWluIEVuZ2luZWVyIi...\n      Full-time\n      NaN\n      NaN\n      NaN\n      block-chain\n    \n    \n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      via LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      [{'title': 'Qualifications', 'items': ['3+ yea...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [24 hours ago, Work from home, Contractor, No ...\n      eyJqb2JfdGl0bGUiOiJCbG9ja2NoYWluIENvdXJzZSBJbn...\n      Contractor\n      True\n      24 hours ago\n      NaN\n      block-chain\n    \n  \n\n\n\n\n\nmerged1 = merge_jobs_results('../../data/2023-04-14-job-search/2023-04-14-job-search-location-USA/') # USA jobs\n\n\nmerged_df = pd.concat([merged, merged1], ignore_index=True) # concatenate DC and USA jobs\nmerged_df.head(3)\n\n\n\n\n\n  \n    \n      \n      title\n      company_name\n      location\n      via\n      description\n      job_highlights\n      related_links\n      extensions\n      job_id\n      detected_extensions.schedule_type\n      detected_extensions.work_from_home\n      detected_extensions.posted_at\n      detected_extensions.salary\n      key\n    \n  \n  \n    \n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      via Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      [{'title': 'Qualifications', 'items': ['2-3 ye...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [Work from home, Full-time, No degree mentioned]\n      eyJqb2JfdGl0bGUiOiJFdGhlcmV1bSBCbG9ja2NoYWluIE...\n      Full-time\n      True\n      NaN\n      NaN\n      block-chain\n    \n    \n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      via Greenhouse\n      We are seeking a highly motivated and skilled ...\n      [{'title': 'Qualifications', 'items': ['Bachel...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [Full-time]\n      eyJqb2JfdGl0bGUiOiJCbG9ja2NoYWluIEVuZ2luZWVyIi...\n      Full-time\n      NaN\n      NaN\n      NaN\n      block-chain\n    \n    \n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      via LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      [{'title': 'Qualifications', 'items': ['3+ yea...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [24 hours ago, Work from home, Contractor, No ...\n      eyJqb2JfdGl0bGUiOiJCbG9ja2NoYWluIENvdXJzZSBJbn...\n      Contractor\n      True\n      24 hours ago\n      NaN\n      block-chain\n    \n  \n\n\n\n\n\ndf_highlights = pd.json_normalize(merged_df['job_highlights']) # break up job highlights\ndf_highlights.head(5)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      {'title': 'Qualifications', 'items': ['2-3 yea...\n      {'title': 'Responsibilities', 'items': ['Desig...\n      None\n    \n    \n      1\n      {'title': 'Qualifications', 'items': ['Bachelo...\n      {'title': 'Responsibilities', 'items': ['As a ...\n      {'title': 'Benefits', 'items': ['(NYC only) Pu...\n    \n    \n      2\n      {'title': 'Qualifications', 'items': ['3+ year...\n      {'title': 'Responsibilities', 'items': ['Our e...\n      None\n    \n    \n      3\n      {'title': 'Qualifications', 'items': ['Candida...\n      {'title': 'Responsibilities', 'items': ['Will ...\n      None\n    \n    \n      4\n      {'title': 'Qualifications', 'items': ['You are...\n      {'title': 'Responsibilities', 'items': ['To bu...\n      None"
  },
  {
    "objectID": "data-cleaning.html#clean-data",
    "href": "data-cleaning.html#clean-data",
    "title": "Data Cleaning",
    "section": "2 Clean Data",
    "text": "2 Clean Data\nNow, we will clean all the necessary columns of our data, and extract relevant data\n\ndef get_items_value(dictionary): # get item value\n    if pd.isnull(dictionary): # deal with na values\n        return None\n    return dictionary['items']\n\ndf_highlights = df_highlights.applymap(get_items_value) # only get the description of the item\n\n\ndf_highlights = df_highlights.rename(columns={0: 'qualifications', 1: 'responsibilities', 2: 'benefits'}) # rename columns\nprint(df_highlights.isna().sum())\n\nqualifications        0\nresponsibilities    105\nbenefits            400\ndtype: int64\n\n\n\nmaster_df = pd.concat([merged_df, df_highlights], axis = 1) # merge both dataframes by column\nmaster_df = master_df.drop(['job_highlights', 'related_links', 'job_id', 'detected_extensions.work_from_home', 'detected_extensions.posted_at'], axis = 1) # drop unneeded columns\n\nfor i in master_df: # check for nas\n    print(i, master_df[i].isna().sum())\n\ntitle 0\ncompany_name 0\nlocation 0\nvia 0\ndescription 0\nextensions 0\ndetected_extensions.schedule_type 1\ndetected_extensions.salary 690\nkey 0\nqualifications 0\nresponsibilities 105\nbenefits 400\n\n\n\n#rename columns\nmaster_df = master_df.rename(columns={'detected_extensions.schedule_type': 'schedule_type', 'detected_extensions.salary': 'salary', 'key': 'query'})\n\nmaster_df = master_df.reset_index(drop=True)\n\nmaster_df.head(5)\n\n\n\n\n\n  \n    \n      \n      title\n      company_name\n      location\n      via\n      description\n      extensions\n      schedule_type\n      salary\n      query\n      qualifications\n      responsibilities\n      benefits\n    \n  \n  \n    \n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      via Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      [Work from home, Full-time, No degree mentioned]\n      Full-time\n      NaN\n      block-chain\n      [2-3 years of Software Development experience,...\n      [Design, maintain and deploy smart contracts f...\n      None\n    \n    \n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      via Greenhouse\n      We are seeking a highly motivated and skilled ...\n      [Full-time]\n      Full-time\n      NaN\n      block-chain\n      [Bachelor's or Master's degree in Computer Sci...\n      [As a Blockchain Engineer, you will be respons...\n      [(NYC only) Pursuant to Section 8-102 of title...\n    \n    \n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      via LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      [24 hours ago, Work from home, Contractor, No ...\n      Contractor\n      NaN\n      block-chain\n      [3+ years of experience in blockchain, cryptoc...\n      [Our expert technical team will provide the su...\n      None\n    \n    \n      3\n      Python based - Blockchain developer to join ex...\n      Upwork\n      Anywhere\n      via Upwork\n      Need someone to join our existing team to spee...\n      [2 days ago, 10–30 an hour, Work from home, Co...\n      Contractor\n      10–30 an hour\n      block-chain\n      [Candidates must be willing to sign, non-discl...\n      [Will discuss details with the selected candid...\n      None\n    \n    \n      4\n      Blockchain DevOps Engineer (Remote)\n      Telnyx\n      United States\n      via Startup Jobs\n      About Telnyx\\n\\nAt Telnyx, we’re architecting ...\n      [4 days ago, Full-time, No degree mentioned]\n      Full-time\n      NaN\n      block-chain\n      [You are a highly motivated and experienced Bl...\n      [To build a best-in-class Filecoin (FIL) Minin...\n      None\n    \n  \n\n\n\n\n\nmaster_df['query'] = master_df['query'].str.replace('-', ' ') # replace hyphens with spaces\nmaster_df.loc[master_df['query'] == 'block chain', 'query'] = 'blockchain' # remove space from blockchain\n\n\nmaster_df['via'] = master_df['via'].str.slice(4) # remove 'via' from the start of the string\n\nmaster_df = master_df.join(master_df['extensions'].apply(lambda x: pd.Series(x)).add_prefix('ext_'))\nmaster_df = master_df.drop('extensions', axis=1)\n\n\nmaster_df.head(5)\n\n\n\n\n\n  \n    \n      \n      title\n      company_name\n      location\n      via\n      description\n      schedule_type\n      salary\n      query\n      qualifications\n      responsibilities\n      benefits\n      ext_0\n      ext_1\n      ext_2\n      ext_3\n      ext_4\n      ext_5\n      ext_6\n      ext_7\n    \n  \n  \n    \n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      Full-time\n      NaN\n      blockchain\n      [2-3 years of Software Development experience,...\n      [Design, maintain and deploy smart contracts f...\n      None\n      Work from home\n      Full-time\n      No degree mentioned\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      Greenhouse\n      We are seeking a highly motivated and skilled ...\n      Full-time\n      NaN\n      blockchain\n      [Bachelor's or Master's degree in Computer Sci...\n      [As a Blockchain Engineer, you will be respons...\n      [(NYC only) Pursuant to Section 8-102 of title...\n      Full-time\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      Contractor\n      NaN\n      blockchain\n      [3+ years of experience in blockchain, cryptoc...\n      [Our expert technical team will provide the su...\n      None\n      24 hours ago\n      Work from home\n      Contractor\n      No degree mentioned\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      Python based - Blockchain developer to join ex...\n      Upwork\n      Anywhere\n      Upwork\n      Need someone to join our existing team to spee...\n      Contractor\n      10–30 an hour\n      blockchain\n      [Candidates must be willing to sign, non-discl...\n      [Will discuss details with the selected candid...\n      None\n      2 days ago\n      10–30 an hour\n      Work from home\n      Contractor\n      No degree mentioned\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Blockchain DevOps Engineer (Remote)\n      Telnyx\n      United States\n      Startup Jobs\n      About Telnyx\\n\\nAt Telnyx, we’re architecting ...\n      Full-time\n      NaN\n      blockchain\n      [You are a highly motivated and experienced Bl...\n      [To build a best-in-class Filecoin (FIL) Minin...\n      None\n      4 days ago\n      Full-time\n      No degree mentioned\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nmaster_df['schedule_type'].unique()\n\narray(['Full-time', 'Contractor', 'Internship', 'Part-time', nan],\n      dtype=object)\n\n\n\n# loop through each row in the dataframe to delete 'days ago' 'hours ago'\nfor i, row in master_df.iterrows():\n    # check if 'ago' is present in ext_0\n    if 'ago' in str(row['ext_0']):\n        # if it is, replace the 'ago' value with the corresponding value from ext_1\n        master_df.at[i, 'ext_0'] = row['ext_1']\n        master_df.at[i, 'ext_1'] = np.nan\n    else:\n        # if it isn't, keep the row as it is\n        pass\n\n\nmaster_df.isna().sum()\n\ntitle                 0\ncompany_name          0\nlocation              0\nvia                   0\ndescription           0\nschedule_type         1\nsalary              690\nquery                 0\nqualifications        0\nresponsibilities    105\nbenefits            400\next_0                 0\next_1               641\next_2               343\next_3               530\next_4               670\next_5               778\next_6               812\next_7               819\ndtype: int64\n\n\n\nmaster_df = master_df.drop(['ext_0', 'ext_1', 'ext_2', 'ext_3', 'ext_4', 'ext_5', 'ext_6', 'ext_7'], axis = 1)\n\n\nfor i, row in master_df.iterrows(): # go through responsibilities and benefits column to extract salary\n    if pd.isna(row['salary']):\n        if '$' in str(row['responsibilities']):\n            master_df.at[i, 'salary'] = row['responsibilities']\n        elif 'hour' in str(row['responsibilities']):\n            master_df.at[i, 'salary'] = row['responsibilities']\n        elif 'year' in str(row['responsibilities']):\n            master_df.at[i, 'salary'] = row['responsibilities']\n        elif '$' in str(row['benefits']):\n            master_df.at[i, 'salary'] = row['benefits']\n        elif 'hour' in str(row['benefits']):\n            master_df.at[i, 'salary'] = row['benefits']\n        elif 'year' in str(row['benefits']):\n            master_df.at[i, 'salary'] = row['benefits']\n\n\nmaster_df['degree'] = np.nan\n\n\nfor i, row in master_df.iterrows(): # go through responsibilities and benefits column to extract salary\n    if pd.isna(row['degree']):\n        if \"PhD\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"PhD\"\n        elif \"Ph.D.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"PhD\"\n        elif \"Doctorate\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"PhD\"\n        elif \"Master\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Master's\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Masters\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Msc\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"M.A.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"MA\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"MS\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Advanced\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"M.S.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Bachelor's\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"BS\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"B.S.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"BA\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"B.A.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"Bachelors\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"Bachelor\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"Undergraduate\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n\n\nmaster_df['salary'] = master_df['salary'].apply(lambda x: str(x) if isinstance(x, list) else x) # convert salary to string \n\n\ndef clean_salary(salary):\n    # if salary is a list, extract the salary range from the appropriate string\n    if isinstance(salary, list):\n        for s in salary:\n            if \"salary range\" in s.lower():\n                salary = re.findall(r\"\\$\\d+(?:,\\d+)*(?:\\.\\d+)?\", s)\n                salary = \" - \".join(salary)\n                break\n    # if salary is a string, extract the salary range\n    elif isinstance(salary, str):\n        salary = re.findall(r\"\\$\\d+(?:,\\d+)*(?:\\.\\d+)?\", salary)\n        salary = \" - \".join(salary)\n    else:\n        salary = np.nan\n    return salary\n\ndef extract_time_period(salary):\n    if isinstance(salary, str):\n        if \"hour\" in salary:\n            time_period = \"an hour\"\n        elif \"month\" in salary:\n            time_period = \"a month\"\n        elif \"year\" in salary:\n            time_period = \"a year\"\n        else:\n            time_period = np.nan\n    else:\n        time_period = np.nan\n    return time_period\n\nmaster_df[\"salary_cleaned\"] = master_df[\"salary\"].apply(clean_salary)\nmaster_df[\"time_period\"] = master_df[\"salary_cleaned\"].apply(extract_time_period)\n\n\n# iterate over every row in the dataframe\nfor index, row in master_df.iterrows():\n    # check if the 'salary_cleaned' value contains '$'\n    if '$' in str(row['salary_cleaned']):\n        # assign the entire value to the 'salary' column\n        master_df.at[index, 'salary'] = row['salary_cleaned'] #####\n\n\n# remove list values from salary\n# create a boolean mask to filter out missing values \nnot_null_mask = pd.notnull(master_df['salary']) \n\n# find the rows where 'salary' contains '['\nrows_with_brackets = master_df[not_null_mask & master_df['salary'].str.contains('\\\\[')]\n\n# set the 'salary' column to NaN for the rows with brackets\nmaster_df.loc[rows_with_brackets.index, 'salary'] = np.nan\n\n\nmaster_df['salary'] = master_df['salary'].str.replace('–', '-')\n\n\n# create a boolean mask to filter for rows containing 'hour'\nmask = master_df['salary'].str.contains('hour', na=False)\n\n# select the rows where 'salary' contains 'hour'\nrows_with_hourly_wage = master_df[mask]\n\n# convert hourly wage to annual salary\nfor idx, row in rows_with_hourly_wage.iterrows():\n    salary_str = row['salary']\n    if salary_str.count('-') == 1:\n        # handle case where salary range is given\n        salary_range = salary_str.split(' ')[0]\n        salary_range = salary_range.replace('\\u2011', '-')\n        start, end = map(float, salary_range.split('-'))\n        avg_salary = (start + end) / 2.0\n        annual_salary = avg_salary * 2080\n        master_df.loc[idx, 'salary'] = '${:,.2f}'.format(annual_salary)\n    elif salary_str.count('-') == 0:\n        # handle case where single hourly wage is given\n        hourly_wage_str = salary_str.split(' ')[0]\n        hourly_wage = float(hourly_wage_str)\n        annual_salary = hourly_wage * 2080\n        master_df.loc[idx, 'salary'] = '${:,.2f}'.format(annual_salary)\n\n\nmaster_df['salary'] = master_df['salary'].str.replace(' a year', '').str.strip()\nmaster_df['salary'] = master_df['salary'].str.replace('$', '').str.strip()\nmaster_df.loc[636, 'salary'] = 136284\nmaster_df.loc[761, 'salary'] = 22070\nmaster_df.loc[681, 'salary'] = 121000\nmaster_df.loc[746, 'salary'] = 206000\n\n/var/folders/37/fsk42jds3255qblrs5r1v99c0000gn/T/ipykernel_82749/4119749001.py:2: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n\n\n\n\nnum_nas = master_df['salary'].isna().sum()\n\nprint(f\"Number of NaN values in 'salary' column: {num_nas}\")\n\nNumber of NaN values in 'salary' column: 473\n\n\n\n# Define a function to convert salary ranges to their average values\nmaster_df['salary'] = master_df['salary'].astype(str)\n\ndef parse_salary_range(s):\n    if '-' in s:\n        s = s.replace('$', '').replace(',', '').replace('K', '000').replace('k', '000')\n        parts = s.split('-')\n        parts = [float(p) for p in parts if not pd.isnull(p)]\n        avg = sum(parts) / len(parts)\n        return avg\n    else:\n        return s\n\n# Apply the function to the 'salary' column of master_df\nmaster_df['salary'] = master_df['salary'].apply(parse_salary_range)\n\n\nmaster_df['salary'] = master_df['salary'].astype(str)  # convert all data points to string type\nmaster_df['salary'] = master_df['salary'].str.replace(',', '')  # remove commas\nmaster_df['salary'] = master_df['salary'].str.replace('\\.\\d+', '')  # remove decimals and everything after them\nmaster_df['salary'] = pd.to_numeric(master_df['salary'], errors='coerce')\nmaster_df['salary'].describe()\n\n/var/folders/37/fsk42jds3255qblrs5r1v99c0000gn/T/ipykernel_82749/1948180007.py:3: FutureWarning:\n\nThe default value of regex will change from True to False in a future version.\n\n\n\ncount       347.000000\nmean     125612.092219\nstd       73116.788777\nmin           0.000000\n25%       59013.000000\n50%      134500.000000\n75%      175000.000000\nmax      400000.000000\nName: salary, dtype: float64\n\n\n\nmaster_df['qualifications'] = master_df['qualifications'].astype(str)\n\ndef extract_experience(text):\n    match = re.search(r'(\\d[\\d+-]*\\s*(?:year|yr|yrs|years))', text, re.IGNORECASE)\n    if match:\n        return match.group(1)\n    else:\n        return None\n\nmaster_df['experience'] = master_df['qualifications'].apply(extract_experience)\n\n\nmaster_df['experience'] = master_df['experience'].apply(lambda x: str(x))\n\ndef clean_experience(exp):\n    if isinstance(exp, str):\n        # Extract digits from the experience string\n        digits = re.findall('\\d+', exp)\n        if len(digits) == 2:\n            # Calculate the average of two numbers if there are two digits\n            avg_exp = (int(digits[0]) + int(digits[1])) / 2\n        elif len(digits) == 1:\n            # Take the single digit if there is only one digit\n            avg_exp = int(digits[0])\n        else:\n            # Return None if there are no digits\n            return None\n        return avg_exp\n    else:\n        # Return None if the input is not a string\n        return None\n\nmaster_df['experience'] = master_df['experience'].apply(clean_experience)\n\n\nmaster_df = master_df.drop(['salary_cleaned', 'time_period'], axis = 1)\n\n\nmaster_df.isna().sum()\n\ntitle                 0\ncompany_name          0\nlocation              0\nvia                   0\ndescription           0\nschedule_type         1\nsalary              476\nquery                 0\nqualifications        0\nresponsibilities    105\nbenefits            400\ndegree              287\nexperience          436\ndtype: int64\n\n\n\nmin_count = master_df.count().min()\nprint(\"Number of rows with entries for every column:\", min_count)\n\nNumber of rows with entries for every column: 347\n\n\n\nmaster_df['degree'] = master_df['degree'].astype('category')\n\n\nmaster_df.dtypes\n\ntitle                 object\ncompany_name          object\nlocation              object\nvia                   object\ndescription           object\nschedule_type         object\nsalary               float64\nquery                 object\nqualifications        object\nresponsibilities      object\nbenefits              object\ndegree              category\nexperience           float64\ndtype: object\n\n\n\nmaster_df['location'] = master_df['location'].str.strip() # delete extra spaces\nmaster_df['remote'] = master_df['location'].apply(lambda x: True if x in ['Anywhere', 'United States'] else False) # make 'remote' dummy variable\n\n\nmaster_df.loc[[26, 58, 73, 77, 127, 230, 271, 322, 463, 472, 651, 731, 763, 768, 781], 'salary'] = np.nan # delete bonuses that were detected as salaries\n\nmaster_df.loc[[48, 110, 193, 264, 614, 654], 'salary'] *= 2080 # convert unconverted hourly salaries\n\nmaster_df.loc[[108, 143, 151, 231, 249, 340, 350, 357, 396, 426, 525, 542, 557, 627, 737, 758, 808], 'salary'] *= 1000 # salaries read as 240 instead of 240000\n\nmaster_df.loc[[251, 696], 'salary'] *= 12 # convert unconverted monthly salaries\n\n\nmaster_df['query'] = master_df['query'].apply(lambda x: ' '.join(word if word == 'and' else word.title() for word in x.split()))\n\n\ndef clean_location(location):\n    # Find the index of the '(' character\n    index = location.find('(')\n    if index != -1:\n        # If '(' is found, remove it and everything that comes after it\n        location = location[:index].strip()\n    # Remove extra spaces at the start and end of the string\n    return location.strip()\n\nmaster_df['location'] = master_df['location'].apply(clean_location)\n\n\n# Define a function to extract city and state information from the location string\ndef get_city_state(location):\n    # Split the location string by ','\n    parts = location.split(',')\n    if len(parts) == 2:\n        # If the location has two parts, assume the first is the city and the second is the state\n        city = parts[0].strip()\n        state = parts[1].strip()\n        return city, state\n    elif len(parts) == 1:\n        # If the location has only one part, assume it is the state\n        state = parts[0].strip()\n        if location == 'Anywhere' or location == 'United States':\n            return '', ''\n        else:\n            return '', state\n    else:\n        # If the location has more than two parts, assume it is not a valid city, state format\n        return '', ''\n\n# Apply the function to the location column\nmaster_df[['city', 'state']] = master_df['location'].apply(lambda x: pd.Series(get_city_state(x)))\n\n# Handle special case for state abbreviations\nmaster_df['state'] = master_df['state'].apply(lambda x: 'TX' if x == 'Texas' else x)\n\n\n# Define a dictionary mapping state abbreviations to their full form\nstate_abbreviations = {\n    'AL': 'Alabama', \n    'AK': 'Alaska', \n    'AZ': 'Arizona', \n    'AR': 'Arkansas', \n    'CA': 'California', \n    'CO': 'Colorado', \n    'CT': 'Connecticut', \n    'DE': 'Delaware', \n    'DC': 'District of Columbia', \n    'FL': 'Florida', \n    'GA': 'Georgia', \n    'HI': 'Hawaii', \n    'ID': 'Idaho', \n    'IL': 'Illinois', \n    'IN': 'Indiana', \n    'IA': 'Iowa', \n    'KS': 'Kansas', \n    'KY': 'Kentucky', \n    'LA': 'Louisiana', \n    'ME': 'Maine', \n    'MD': 'Maryland', \n    'MA': 'Massachusetts', \n    'MI': 'Michigan', \n    'MN': 'Minnesota', \n    'MS': 'Mississippi', \n    'MO': 'Missouri', \n    'MT': 'Montana', \n    'NE': 'Nebraska', \n    'NV': 'Nevada', \n    'NH': 'New Hampshire', \n    'NJ': 'New Jersey', \n    'NM': 'New Mexico', \n    'NY': 'New York', \n    'NC': 'North Carolina', \n    'ND': 'North Dakota', \n    'OH': 'Ohio', \n    'OK': 'Oklahoma', \n    'OR': 'Oregon', \n    'PA': 'Pennsylvania', \n    'RI': 'Rhode Island', \n    'SC': 'South Carolina', \n    'SD': 'South Dakota', \n    'TN': 'Tennessee', \n    'TX': 'Texas', \n    'UT': 'Utah', \n    'VT': 'Vermont', \n    'VA': 'Virginia', \n    'WA': 'Washington', \n    'WV': 'West Virginia', \n    'WI': 'Wisconsin', \n    'WY': 'Wyoming'\n}\n\n# Apply the mapping to the 'state' column\nmaster_df['state'] = master_df['state'].apply(lambda x: state_abbreviations.get(x, x))"
  },
  {
    "objectID": "data-cleaning.html#output-cleaned-csv",
    "href": "data-cleaning.html#output-cleaned-csv",
    "title": "Data Cleaning",
    "section": "3 Output Cleaned CSV",
    "text": "3 Output Cleaned CSV\n\nmaster_df.to_csv('job_data.csv')"
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf = pd.read_csv('../../data/job_data.csv')"
  },
  {
    "objectID": "data-exploration.html#summary-statistics",
    "href": "data-exploration.html#summary-statistics",
    "title": "Data Exploration",
    "section": "2 Summary Statistics",
    "text": "2 Summary Statistics\nThe first step of EDA is to alway look at the structure of our dataframe, and key statistics of both numerical and categorical variables. Our dataset on jobs is one which has several types of data, which is why there is a breadth of exploratory data analysis that can be conducted on this particular dataset. Summary statistics are an important aspect of exploratory data analysis because they provide a concise and comprehensive summary of the key features and characteristics of our data set. By calculating summary statistics, we can quickly gain insights into the central tendency, variability, and distribution of the data.\n\ndf.shape\n\n(823, 15)\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      title\n      company_name\n      location\n      via\n      description\n      schedule_type\n      salary\n      query\n      qualifications\n      responsibilities\n      benefits\n      degree\n      experience\n      remote\n    \n  \n  \n    \n      0\n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      Full-time\n      NaN\n      blockchain\n      ['2-3 years of Software Development experience...\n      ['Design, maintain and deploy smart contracts ...\n      NaN\n      NaN\n      2.5\n      True\n    \n    \n      1\n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      Greenhouse\n      We are seeking a highly motivated and skilled ...\n      Full-time\n      180000.0\n      blockchain\n      [\"Bachelor's or Master's degree in Computer Sc...\n      ['As a Blockchain Engineer, you will be respon...\n      ['(NYC only) Pursuant to Section 8-102 of titl...\n      Master's\n      NaN\n      False\n    \n    \n      2\n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      Contractor\n      NaN\n      blockchain\n      ['3+ years of experience in blockchain, crypto...\n      ['Our expert technical team will provide the s...\n      NaN\n      NaN\n      3.0\n      True\n    \n    \n      3\n      3\n      Python based - Blockchain developer to join ex...\n      Upwork\n      Anywhere\n      Upwork\n      Need someone to join our existing team to spee...\n      Contractor\n      41600.0\n      blockchain\n      ['Candidates must be willing to sign, non-disc...\n      ['Will discuss details with the selected candi...\n      NaN\n      NaN\n      NaN\n      True\n    \n    \n      4\n      4\n      Blockchain DevOps Engineer (Remote)\n      Telnyx\n      United States\n      Startup Jobs\n      About Telnyx\\n\\nAt Telnyx, we’re architecting ...\n      Full-time\n      NaN\n      blockchain\n      ['You are a highly motivated and experienced B...\n      ['To build a best-in-class Filecoin (FIL) Mini...\n      NaN\n      Bachelor's\n      NaN\n      True\n    \n  \n\n\n\n\n\ndf.dtypes\n\nUnnamed: 0            int64\ntitle                object\ncompany_name         object\nlocation             object\nvia                  object\ndescription          object\nschedule_type        object\nsalary              float64\nquery                object\nqualifications       object\nresponsibilities     object\nbenefits             object\ndegree               object\nexperience          float64\nremote                 bool\ndtype: object\n\n\n\ndf['experience'].describe()\n\ncount    387.000000\nmean       4.680879\nstd        2.908509\nmin        1.000000\n25%        3.000000\n50%        4.000000\n75%        5.000000\nmax       20.000000\nName: experience, dtype: float64\n\n\n\ndf['salary'].describe()\n\ncount       332.000000\nmean     143013.671687\nstd       61496.830081\nmin       22070.000000\n25%      107925.000000\n50%      148887.500000\n75%      180412.500000\nmax      400000.000000\nName: salary, dtype: float64\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 823 entries, 0 to 822\nData columns (total 15 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Unnamed: 0        823 non-null    int64  \n 1   title             823 non-null    object \n 2   company_name      823 non-null    object \n 3   location          823 non-null    object \n 4   via               823 non-null    object \n 5   description       823 non-null    object \n 6   schedule_type     822 non-null    object \n 7   salary            332 non-null    float64\n 8   query             823 non-null    object \n 9   qualifications    823 non-null    object \n 10  responsibilities  718 non-null    object \n 11  benefits          423 non-null    object \n 12  degree            536 non-null    object \n 13  experience        387 non-null    float64\n 14  remote            823 non-null    bool   \ndtypes: bool(1), float64(2), int64(1), object(11)\nmemory usage: 90.9+ KB\n\n\n\n(df.isnull().sum()/(len(df)))*100 # percent of missing data\n\nUnnamed: 0           0.000000\ntitle                0.000000\ncompany_name         0.000000\nlocation             0.000000\nvia                  0.000000\ndescription          0.000000\nschedule_type        0.121507\nsalary              59.659781\nquery                0.000000\nqualifications       0.000000\nresponsibilities    12.758202\nbenefits            48.602673\ndegree              34.872418\nexperience          52.976914\nremote               0.000000\ndtype: float64\n\n\n\ncat_cols=df.select_dtypes(include=['object']).columns\nnum_cols = df.select_dtypes(include=np.number).columns.tolist()\nprint(\"Categorical Variables:\")\nprint(cat_cols)\nprint(\"Numerical Variables:\")\nprint(num_cols)\n\nCategorical Variables:\nIndex(['title', 'company_name', 'location', 'via', 'description',\n       'schedule_type', 'query', 'qualifications', 'responsibilities',\n       'benefits', 'degree'],\n      dtype='object')\nNumerical Variables:\n['Unnamed: 0', 'salary', 'experience']"
  },
  {
    "objectID": "data-exploration.html#univariate-and-bivariate-analysis",
    "href": "data-exploration.html#univariate-and-bivariate-analysis",
    "title": "Data Exploration",
    "section": "3 Univariate and Bivariate Analysis",
    "text": "3 Univariate and Bivariate Analysis\nAnother important piece of EDA is to explore the raw data visually, either by means of Univariate or Bivariate analysis. Univariate analysis entails viewing the distribution and features of one variable. It helps us understand the distribution of a single variable, such as the frequency of each value or the range of values in our data. Bivariate analysis, on the other hand, relates to plotting the relationship between two variables. It helps us understand the relationship between two variables, such as correlation or association. Bivariate plots such as scatter plots, line plots, and heatmaps can be used to identify patterns, trends, and dependencies in the data.\n\n# create a histogram of the data\nplt.hist(df['salary'], bins=30, density=True, alpha=0.5, color='green')\n\n# add a title and axis labels\nplt.title('Histogram of Salaries')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\n\n# show the plot\nplt.show()\n\n\n\n\n\n# create a histogram of the data\nplt.hist(df['experience'], density=True, alpha=0.5, color='green')\n\n# add a title and axis labels\nplt.title('Histogram of Experience')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\n\n# show the plot\nplt.show()\n\n\n\n\n\nsns.boxplot(x='remote', y='salary', data=df)\n\n# add a title and axis labels\nplt.title('Boxplot of Salary by Remote Work Status')\nplt.xlabel('Remote Work Status')\nplt.ylabel('Salary')\n\n# show the plot\nplt.show()\n\n\n\n\n\nsns.boxplot(x='degree', y='salary', data=df)\n\n# add a title and axis labels\nplt.title('Boxplot of Salary by Degree')\nplt.xlabel('Degree')\nplt.ylabel('Salary')\n\n# show the plot\nplt.show()\n\n\n\n\n\nsns.boxplot(x='remote', y='experience', data=df)\n\n# add a title and axis labels\nplt.title('Boxplot of Experience by Remote Work Status')\nplt.xlabel('Remote Work Status')\nplt.ylabel('Experience')\n\n# show the plot\nplt.show()\n\n\n\n\n\n# create the plot\nsns.scatterplot(x='experience', y='salary', hue = 'degree', data=df)\n\n# add a title and axis labels\nplt.title('Correlational Plot of Experience and Salary')\nplt.xlabel('Experience')\nplt.ylabel('Salary')\n\n# show the plot\nplt.show()\n\n\n\n\n\nsns.countplot(data = df, x = 'degree')\n\n<AxesSubplot: xlabel='degree', ylabel='count'>\n\n\n\n\n\n\nsns.countplot(data = df, x = 'remote')\n\n<AxesSubplot: xlabel='remote', ylabel='count'>\n\n\n\n\n\n\nsns.boxplot(data = df, x='query', y='salary')\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n [Text(0, 0, 'blockchain'),\n  Text(1, 0, 'natural language processing'),\n  Text(2, 0, 'big data and cloud computing'),\n  Text(3, 0, 'data analyst'),\n  Text(4, 0, 'machine learning'),\n  Text(5, 0, 'reinforcement learning'),\n  Text(6, 0, 'neural networks'),\n  Text(7, 0, 'deep learning'),\n  Text(8, 0, 'data scientist'),\n  Text(9, 0, 'time series'),\n  Text(10, 0, 'time series analysis')])\n\n\n\n\n\n\nsns.boxplot(data = df, x='query', y='experience')\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n [Text(0, 0, 'blockchain'),\n  Text(1, 0, 'natural language processing'),\n  Text(2, 0, 'big data and cloud computing'),\n  Text(3, 0, 'data analyst'),\n  Text(4, 0, 'machine learning'),\n  Text(5, 0, 'reinforcement learning'),\n  Text(6, 0, 'neural networks'),\n  Text(7, 0, 'deep learning'),\n  Text(8, 0, 'data scientist'),\n  Text(9, 0, 'time series'),\n  Text(10, 0, 'time series analysis')])\n\n\n\n\n\n\ndf['company_name'].value_counts().head(10)\n\nUpwork                 56\nBooz Allen Hamilton    17\nApple                  15\nDeloitte               10\nWalmart                 9\nJobot                   8\nSnap Inc.               8\nLockheed Martin         7\nMicrosoft               7\nLeidos                  7\nName: company_name, dtype: int64\n\n\n\ndf['via'].value_counts().head(10)\n\nLinkedIn          66\nZipRecruiter      60\nUpwork            56\nAngelList         36\nLever             23\nGreenhouse        19\nClearance Jobs    19\nStartup Jobs      16\nBuilt In          15\nSalary.com        14\nName: via, dtype: int64\n\n\n\n# get the top 10 most frequent job titles\ntop_5_titles = df['title'].value_counts().head(5)\n\n# create a bar plot of the job title counts\nsns.barplot(x=top_5_titles, y=top_5_titles.index)\nplt.title('Top 10 Job Titles')\nplt.xlabel('Count')\nplt.ylabel('Job Title')\nplt.show()\n\n\n\n\n\nsns.heatmap(df[['salary', 'experience']].corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\nsns.kdeplot(data=df, x='experience')\nplt.title('Distribution of Experience')\nplt.show()\n\n\n\n\n\n# create a bar plot of degree counts\nsns.countplot(data=df, x='degree')\nplt.title('Degree Counts')\nplt.show()\n\n\n\n\n\ntop_8_locations = df['location'].value_counts().head(8)\n\n# create a bar plot of the job title counts\nsns.barplot(x=top_8_locations, y=top_8_locations.index)\nplt.title('Top 8 Job Locations')\nplt.xlabel('Count')\nplt.ylabel('Job Location')\nplt.show()"
  }
]